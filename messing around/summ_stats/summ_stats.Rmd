---
title: "Descriptive facts for work culture GitHub project"
author:
  name: Jacob Toner Gosselin
date: "`r format(Sys.time(), '%d %B %Y')`"

output: 
  html_document:
    theme: flatly
    highlight: haddock
    # code_folding: show
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: false
    keep_tex: false ## Change to true if want keep intermediate .tex file
    css: css/preamble.css ## For multi-col environments
always_allow_html: true
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362
## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, dpi=300)
## Next hook based on this SO answer: https://stackoverflow.com/a/39025054
knitr::knit_hooks$set(
  prompt = function(before, options, envir) {
    options(
      prompt = if (options$engine %in% c('sh','bash')) '$ ' else 'R> ',
      continue = if (options$engine %in% c('sh','bash')) '$ ' else '+ '
      )
    })

```

# Intro
This doc has the code and results for each variable outlined [here](https://docs.google.com/spreadsheets/d/1c49ib2oDodITWm66JfoQ4nshZbZ-YrHiZI8S4D1esHE/edit#gid=0). Results are being stored as CSVs for now (presumably we'll have a different file management system in place; this is just a placeholder). In addition, graphs/summary statistics are included for each variable. 

Each section is dedicated to a specific variable. I include the SQL query used, so it's easy to see if/where I'm making mistakes. **All queries are currently limited to January, 2015** (this is easily expandable, just limiting them for now). Details regarding my connection to Google BigQuery can be seen below.

```{r}
#adding libraries
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, DBI, duckdb, bigrquery, hrbrthemes, nycflights13, glue, Hmisc)
## My preferred ggplot2 theme (optional)
theme_set(hrbrthemes::theme_ipsum())

#setting up BQ connection
billing_id = Sys.getenv("GCE_DEFAULT_PROJECT_ID")

bq_con =
  dbConnect(
    bigrquery::bigquery(),
    project = "ghtorrent-bq",
    dataset = "ght",
    billing = billing_id
  )

bq_con
```

# Variable 1: Number of distinct projects working on
Our first variable is the "number of distinct projects working on", observed at the person-week level, to measure being on multiple projects simultaneously. The query itself can be seen below.
```{r}
var1_query =
  glue_sql(
    "
    SELECT author_id, EXTRACT(WEEK FROM date(created_at)) as week, COUNT(DISTINCT project_id) as num_projects  
    FROM `ghtorrent-bq.ght.commits`
    WHERE EXTRACT(YEAR FROM date(created_at)) = 2015 AND EXTRACT(MONTH FROM date(created_at)) = 1
    group by author_id, EXTRACT(WEEK FROM date(created_at))

    
    "
  )
print(var1_query)
```

I pull and store this data as a CSV. Distribution statistics and histograms for "num_projects" (i.e. the number of distinct projects a user worked on in a given week; unit of observation is person-week) can be seen below.
```{r, include=FALSE}
#pulling down the data (commented out since once I store it as a CSV I pull from that to avoid recharging Google BigQuery)
var1_data = dbGetQuery(bq_con, var1_query)
write.csv(var1_data, "./data/var1_data.csv")
```
```{r, warning=FALSE, message=FALSE}
var1_data = read.csv("./data/var1_data.csv")
describe(var1_data$num_projects)
var1_data$week.f = factor(var1_data$week)
var1_data %>% 
  ggplot(aes(x = num_projects, color = week.f)) +
  geom_histogram(position = "identity", alpha = 0.5, fill = "white") + 
  xlim(0, 5) + labs(title = "Distribution of distinct projects worked on (1-5)", x = "Number of distinct projects worked on (by person-week)")

var1_data %>% 
  ggplot(aes(x = num_projects, color = week.f)) +
  geom_histogram(position = "identity", alpha = 0.5, fill = "white") + 
  xlim(10, 100) + labs(title = "Distribution of distinct projects worked on (10-100)", x = "Number of distinct projects worked on (by person-week)")
```

# Variable 2: percent of team members for which this is only team we share
Our second variable is the percent of project members for which this is the only project they share (unit of observation is person-project-week). This is a measure of how disconnected the projects are. I first query person-project-week combinations, with counts of the number of distinct projects they worked on that week. The query can be seen below. 

```{r}
var2_query =
  glue_sql(
    "
      SELECT DISTINCT author_id, project_id, EXTRACT(WEEK FROM date(created_at)) as week
      FROM `ghtorrent-bq.ght.commits`
      WHERE EXTRACT(YEAR FROM date(created_at)) = 2015 AND EXTRACT(MONTH FROM date(created_at)) = 1 
      GROUP BY author_id, project_id, EXTRACT(WEEK FROM date(created_at)) 


    "
  )
print(var2_query)
```

I pull and store this data as a CSV. I then subset the data to only include observations with num_projects >= 2, then lapply a function to create dummy variables for whether the observation (person-project-week) represents the only project shared by co-editors. **Flagging because I need some help here; I believe I've written a function that satisfies what we want, but it would take forever to run (I run it over only ten observations now and it takes a second; given that it increases linearly, it's unfeasible to run this over the 1.1 million observations we have from January 2015)**. 

```{r}
#pulling down the data (commented out since once I store it as a CSV I pull from that to avoid recharging Google BigQuery)
# var2_data = dbGetQuery(bq_con, var2_query)
# write.csv(var2_data, "./data/var2_data.csv")
```
```{r, echo = TRUE}
var2_data = read.csv("./data/var2_data.csv")
var2_data = merge(var1_data, var2_data, by = c("author_id", "week"))
var2_data_sub = subset(var2_data, num_projects > 1)

get_mult_team = function(x, y, z){
  author_list1 = subset(var2_data_sub, week == y & project_id == z)
  author_list1 = subset(author_list1, author_list1 != x)
  proj_list = subset(var2_data_sub, author_id == x & week == y)$project_id
  proj_list = subset(proj_list, proj_list != z)
  author_list2 = subset(var2_data_sub, project_id %in% proj_list & week == y)$author_id
  author_list2 = subset(author_list2, author_list2 != x)
  return(ifelse(sum(author_list1 %in% author_list2) > 0, 1, 0))
}

test = var2_data_sub[c(1:10),]
test2 = mapply(get_mult_team, x = test$author_id, y = test$week, z = test$project_id)
```

# Variable 3: Number of new teams joined
Our third variable is number of new teams joined (unit of observation is person-year). This is a measure of being on multiple teams serially. The query itself can be seen below. **NOTE: I don't limit my pull this time, because our unit of observation makes the data manageable**. 
```{r}
var3_query =
  glue_sql(
    "
    SELECT follower_id, EXTRACT(YEAR FROM date(created_at)) as year, COUNT(DISTINCT user_id) as num_projects   
    FROM `ghtorrent-bq.ght.followers` 
    group by follower_id, EXTRACT(YEAR FROM date(created_at))

    "
  )
print(var3_query)
```
I pull and store this data as a CSV. Distribution statistics and histograms for "num_projects" (i.e. the number of new projects a user worked on in a given year; unit of observation is person-year) can be seen below.
```{r, include = FALSE}
#pulling down the data (commented out since once I store it as a CSV I pull from that to avoid recharging Google BigQuery)
# var3_data = dbGetQuery(bq_con, var3_query)
# write.csv(var3_data, "./data/var3_data.csv")
```
```{r, warning=FALSE, message=FALSE}
var3_data = read.csv("./data/var3_data.csv")
describe(var3_data$num_projects)
var3_data$year.f = factor(var3_data$year)
var3_data %>% 
  ggplot(aes(x = num_projects, color = year.f)) +
  geom_histogram(position = "identity", alpha = 0.5, fill = "white") + 
  xlim(0, 5) + labs(title = "Distribution of new projects worked on (1-5)", x = "Number of distinct projects worked on (by person-year)")

var3_data %>% 
  ggplot(aes(x = num_projects, color = year.f)) +
  geom_histogram(position = "identity", alpha = 0.5, fill = "white") + 
  xlim(10, 250) + labs(title = "Distribution of new projects worked on (10-250)", x = "Number of distinct projects worked on (by person-year)")
```

# Variable 4: Number of distinct authors working on project in given hour
Our fourth variable is number of distinct authors working on project in given hour (unit of observation is project-hour). This is a measure of how much work is synchronous. The query itself can be seen below.  
```{r}
var4_query =
  glue_sql(
    "
    SELECT project_id, EXTRACT(WEEK from created_at) as week, EXTRACT(DAY from created_at) as day, EXTRACT(HOUR from created_at) as hour, COUNT(DISTINCT author_id) as num_authors  
    FROM `ghtorrent-bq.ght.commits`
    WHERE EXTRACT(YEAR FROM date(created_at)) = 2015 AND EXTRACT(MONTH FROM date(created_at)) = 1
    group by project_id, EXTRACT(WEEK from created_at), EXTRACT(DAY FROM created_at), EXTRACT(HOUR FROM created_at)

    "
  )
print(var4_query)
```
I pull and store this data as a CSV. Distribution statistics and histograms for "num_projects" (i.e. the number of new projects a user worked on in a given year; unit of observation is person-year) can be seen below.
```{r, include = FALSE}
#pulling down the data (commented out since once I store it as a CSV I pull from that to avoid recharging Google BigQuery)
# var4_data = dbGetQuery(bq_con, var4_query)
# write.csv(var4_data, "./data/var4_data.csv")
var4_data = dbGetQuery(bq_con, var4_query)
```
```{r, warning=FALSE, message=FALSE}
describe(var4_data$num_authors)
number = sum(var4_data$num_authors > 1)
percent = sum(var4_data$num_authors > 1)/nrow(var4_data)
print(paste("Number of synchronous project hours:", number, sep = " "))
print(paste("Percent of project hours synchronous:", percent, sep = " "))

var4_data$week.f = factor(var4_data$week)
var4_data %>% 
  ggplot(aes(x = num_authors, color = week.f)) +
  geom_histogram(position = "identity", alpha = 0.5, fill = "white") + 
  xlim(0, 5) + labs(title = "Distribution of syhcronous work (by project-hour)", x = "Number of distinct authors working on project in an hour (0-5)")

var4_data %>% 
  ggplot(aes(x = num_authors, color = week.f)) +
  geom_histogram(position = "identity", alpha = 0.5, fill = "white") + 
  xlim(10, 100) + labs(title = "Distribution of syhcronous work (by project-hour)", x = "Number of distinct authors working on project in an hour (10-100)")
```

# Variable 5: Share of team members who joined partway through
Our fifth variable is number of distinct authors working on project who joined after the project was originally created. This is a measure of variation in team composition within its lifetime.  
```{r}
var5_query =
  glue_sql(
    "
    SELECT repo_id, COUNT(DISTINCT user_id) as project_members, COUNT(DISTINCT date(created_at)) as late_additions 
    FROM `ghtorrent-bq.ght.project_members` 
    WHERE EXTRACT(YEAR FROM date(created_at)) = 2015
    group by repo_id

    "
  )
print(var5_query)
```
I pull and store this data as a CSV. Distribution statistics and histograms for "num_projects" (i.e. the number of new projects a user worked on in a given year; unit of observation is person-year) can be seen below.
```{r, include = FALSE}
#pulling down the data (commented out since once I store it as a CSV I pull from that to avoid recharging Google BigQuery)
# var5_data = dbGetQuery(bq_con, var5_query)
# write.csv(var5_data, "./data/var5_data.csv")
```
```{r, warning=FALSE, message=FALSE}
var5_data = read.csv("./data/var5_data.csv")
var5_data$perc_late = var5_data$late_additions/var5_data$project_members
describe(var5_data$perc_late)

var5_data %>% 
  ggplot(aes(x = perc_late)) +
  geom_histogram(position = "identity", alpha = 0.5) + 
  xlim(0,1) + labs(title = "Distribution of ``late addition'' project percentage", x = "Percent of project members added after project creation date")
```